/**
 * CoworkAny GUI 手动测试指南
 * 
 * 本文件提供通过 Desktop GUI 手动验证各能力的测试用例。
 * 自动化测试通过 IPC 模拟了 GUI 的行为，但某些功能需要手动 GUI 测试来验证。
 * 
 * 使用方法：
 * 1. 启动 Desktop: cd desktop && bun run tauri dev
 * 2. 在 GUI 输入框中输入对应的测试指令
 * 3. 观察结果是否符合预期
 * 
 * Run: 无需运行，本文件为测试指南
 */

# CoworkAny GUI 手动测试用例

## 测试准备

### 1. 启动 CoworkAny Desktop

```bash
# 终端1: 启动 Sidecar (如果 Desktop 没有自动启动)
cd D:\private\coworkany\sidecar
bun run src/main.ts

# 终端2: 启动 Desktop GUI
cd D:\private\coworkany\desktop
bun run tauri dev
```

### 2. 等待 GUI 启动

- 桌面窗口弹出
- 输入框可用

---

## 测试用例

### T-01: 自我纠错能力测试

**GUI输入:**
```
请列出XYZ不存在的目录12345的内容
```

**预期行为:**
1. Agent尝试列出目录
2. 返回错误（目录不存在）
3. Agent尝试其他方式或给出错误说明

**验证要点:**
- 工具调用记录显示 list_dir 被调用
- 错误被正确处理和报告

---

### T-02: 任务分解能力测试

**GUI输入:**
```
帮我完成以下任务：
1. 创建一个文件 test_decompose.txt，内容是"测试内容"
2. 读取该文件
3. 列出目录确认文件存在
```

**预期行为:**
1. AutonomousAgent 被触发
2. 任务被分解为多个子任务
3. 依次执行：write_to_file → view_file → list_dir

**验证要点:**
- 日志显示 "AutonomousAgent" 相关事件
- 多个工具按顺序被调用

---

### T-03: 多工具链式编排测试

**GUI输入:**
```
搜索Python的最新动态，然后创建一个报告文件保存搜索结果
```

**预期行为:**
1. 调用 search_web 搜索
2. 调用 write_to_file 保存结果
3. 报告文件被创建

**验证要点:**
- search_web 被调用
- write_to_file 被调用
- 文件被成功创建

---

### T-04: 代码执行测试

**GUI输入:**
```
写一个Python程序计算1+2+3+...+100的总和，然后运行它
```

**预期行为:**
1. write_to_file 创建Python文件
2. run_command 运行Python代码
3. 返回计算结果 5050

**验证要点:**
- .py 文件被创建
- run_command 返回 5050

---

### T-05: 记忆持久化测试

**GUI输入:**
```
记住我的名字叫"测试用户小明"，然后问我叫什么名字
```

**预期行为:**
1. 调用 remember 保存记忆
2. 调用 recall 召回记忆
3. 回答包含"测试用户小明"

**验证要点:**
- 工具调用包含 remember/recall
- 回答正确

---

### T-06: ReAct推理测试

**GUI输入:**
```
如果今天是星期三，那么后天是星期几？请给出推理过程
```

**预期行为:**
1. Agent进行推理
2. 回答：后天是星期五
3. 显示推理过程

**验证要点:**
- 回答包含"星期五"
- 推理过程清晰

---

### T-07: 主动学习测试

**GUI输入:**
```
学习这个新的API端点: GET /api/users 返回用户列表，包含id和name字段
```

**预期行为:**
1. trigger_learning 被调用
2. 知识被存储

**验证要点:**
- 工具调用包含 trigger_learning

---

### T-08: 技能管理测试

**GUI输入:**
```
列出所有可用的技能和它们的版本
```

**预期行为:**
1. 返回技能列表
2. 包含版本信息

**验证要点:**
- 列出多个技能

---

### T-09: 置信度追踪测试

**GUI输入:**
```
预测2026年人工智能发展的三个趋势，并说明你的置信度
```

**预期行为:**
1. 提供预测
2. 标注置信度（如：高/中/低）

**验证要点:**
- 回答包含置信度描述

---

### T-10: Email操作测试

**GUI输入:**
```
检查我的邮箱，看有没有新邮件
```

**预期行为:**
1. 调用 email_check
2. 返回邮件列表或"无新邮件"

**验证要点:**
- 工具调用包含 email_check
- 返回结果

---

### T-11: GitHub操作测试

**GUI输入:**
```
列出我的GitHub仓库
```

**预期行为:**
1. 调用 list_repos
2. 返回仓库列表

**验证要点:**
- 工具调用包含 list_repos
- 返回仓库列表

---

### T-12: 学习预测测试

**GUI输入:**
```
分析之前的学习记录，预测下次应该学习什么
```

**预期行为:**
1. get_learning_predictions 被调用
2. 返回预测建议

**验证要点:**
- 工具调用包含 get_learning_predictions

---

### T-13: PPT制作测试（需要配置搜索API）

**GUI输入:**
```
帮我制作一个关于"2026年AI在智慧城市中的发展"的PPT，包含：
1. 智慧城市的定义
2. AI在智慧城市中的应用
3. 2026年最新趋势
4. 成功案例
5. 未来展望
```

**预期行为:**
1. 搜索相关信息（需要配置 TAVILY_API_KEY）
2. 安装 python-pptx（如需要）
3. 创建 PPT 文件

**验证要点:**
- search_web 被调用
- python-pptx 被安装
- .pptx 文件被创建

---

## 自动化测试 vs 手动测试

| 测试方式 | 优点 | 缺点 |
|---------|------|------|
| 自动化测试 (IPC) | 可重复、速度快 | 无法验证GUI渲染 |
| 手动测试 (GUI) | 验证完整用户体验 | 耗时、不可重复 |

### 建议

1. **自动化测试**: 验证核心功能正常工作
2. **手动GUI测试**: 验证用户体验和GUI交互

---

## 配置搜索API（如需要完整测试）

免费搜索服务经常限流，建议配置：

```bash
# Windows
set TAVILY_API_KEY=your_key_here

# 或创建 .env 文件在 sidecar 目录
TAVILY_API_KEY=your_key_here
```

获取 API key: https://tavily.com/
